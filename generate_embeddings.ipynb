{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84956469",
   "metadata": {},
   "source": [
    "# Generate Embeddings by BAAI/BGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import json\n",
    "def make_descriptor_sentence(descriptor):\n",
    "    if descriptor.startswith('a') or descriptor.startswith('an'):\n",
    "        return f\"which is {descriptor}\"\n",
    "    elif descriptor.startswith('has') or descriptor.startswith('often') or descriptor.startswith('typically') or descriptor.startswith('may') or descriptor.startswith('can'):\n",
    "        return f\"which {descriptor}\"\n",
    "    elif descriptor.startswith('used'):\n",
    "        return f\"which is {descriptor}\"\n",
    "    else:\n",
    "        return f\"which has {descriptor}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take ade20k for example.\n",
    "# Change this to generate embeddings for other datasets. \n",
    "# Also set proper clusters number for different datasets.\n",
    "dataset_name = 'ade20k'\n",
    "\n",
    "with open(f\"./descriptors/descriptors_{dataset_name}_gpt3.5_cluster.json\") as json_file:\n",
    "    descriptions = json.load(json_file)\n",
    "num_classes = len(tuple(descriptions.keys()))\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3b97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ade_desc_prompt_template_embeddings = dict()\n",
    "# Load model from HuggingFace Hub\n",
    "model_name = \"bge-base-en-v1.5\"\n",
    "model_name_prefix = \"BAAI/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_prefix + model_name)\n",
    "model = AutoModel.from_pretrained(model_name_prefix + model_name).to(device)\n",
    "model.eval()\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "all_descriptions_embeddings = []\n",
    "for class_name, desc in descriptions.items():\n",
    "    \n",
    "    sentences = [item.lower() for item in desc]\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').to(device)    \n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "    \n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1).cpu()\n",
    "        all_descriptions_embeddings.append(sentence_embeddings)\n",
    "all_descriptions_embeddings_tensor = torch.cat(all_descriptions_embeddings, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06a2b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose proper cluster number for different datasets. We set 256 for ADE20K.\n",
    "n_clusters = 256\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=\"auto\").fit(all_descriptions_embeddings_tensor.numpy())\n",
    "desc_class_idx_list = []\n",
    "for class_idx, class_desc in enumerate(all_descriptions_embeddings):\n",
    "    desc_class_idx_list.append(torch.tensor([class_idx] * len(class_desc)))\n",
    "desc_class_idx_list = torch.cat(desc_class_idx_list)\n",
    "ground_truth_all_classes = []\n",
    "activated_clusters_all_classes = []\n",
    "for class_idx in range(num_classes):\n",
    "    class_cluster_info = kmeans.labels_[desc_class_idx_list == class_idx]\n",
    "    activated_clusters = np.unique(class_cluster_info)\n",
    "    print(activated_clusters)\n",
    "    activated_clusters_all_classes.append(activated_clusters)\n",
    "    ground_truth = np.zeros(n_clusters)\n",
    "    ground_truth[activated_clusters] = 1\n",
    "    ground_truth_all_classes.append(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef6588-ce86-4cab-bf6c-8aee390a00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label, unique_count =  np.unique(np.array(ground_truth_all_classes), axis=0, return_counts=True)\n",
    "print(unique_count)\n",
    "if not (unique_count == 1).all():\n",
    "    confused_labels = np.where((np.array(ground_truth_all_classes) == unique_label[unique_count>1]).all(axis=1))\n",
    "    for class_idx in confused_labels[0]:\n",
    "        print('----------------')\n",
    "        print(tuple(descriptions.keys())[class_idx])\n",
    "        print(descriptions[tuple(descriptions.keys())[class_idx]])\n",
    "        print('----------------')\n",
    "\n",
    "ground_truth_all_classes = torch.tensor(np.array(ground_truth_all_classes).transpose(1, 0))\n",
    "\n",
    "cluster_embedding_bank = kmeans.cluster_centers_.transpose(1, 0)\n",
    "cluster_embedding_bank = torch.tensor(cluster_embedding_bank)\n",
    "\n",
    "cluster_bank = [cluster_embedding_bank, ground_truth_all_classes]\n",
    "\n",
    "cluster_dict = {'descriptions': descriptions}\n",
    "cluster_dict[f'{model_name}_gpt3.5_cluster_{n_clusters}_embeddings_and_labels'] = cluster_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec27e15-4f76-4d8f-9efa-930e4062101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cluster_dict, f'./embeddings/{dataset_name}_desc_{model_name}_gpt3.5_cluster_{n_clusters}_embedding_bank.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
